Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /home/ds7000/.local/lib/python3.8/site-packages (1.3.2)
Requirement already satisfied: joblib>=1.1.1 in /home/ds7000/.local/lib/python3.8/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: scipy>=1.5.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/scipy-1.5.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.5.2)
Requirement already satisfied: numpy<2.0,>=1.17.3 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.19.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ds7000/.local/lib/python3.8/site-packages (from scikit-learn) (3.2.0)
WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.
You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.
Files already downloaded and verified
Files already downloaded and verified
/home/ds7000/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch [0], last_lr: 0.00007, train_loss: 2.1378, val_loss: 3.8887, val_acc: 0.0963, f1: 0.0963
Epoch [1], last_lr: 0.00015, train_loss: 1.8346, val_loss: 3.3985, val_acc: 0.1799, f1: 0.1799
Epoch [2], last_lr: 0.00028, train_loss: 1.6309, val_loss: 3.1295, val_acc: 0.2465, f1: 0.2465
Epoch [3], last_lr: 0.00044, train_loss: 1.4743, val_loss: 2.8975, val_acc: 0.2828, f1: 0.2828
Epoch [4], last_lr: 0.00060, train_loss: 1.3212, val_loss: 2.8699, val_acc: 0.2918, f1: 0.2918
Epoch [5], last_lr: 0.00076, train_loss: 1.2091, val_loss: 2.4522, val_acc: 0.3626, f1: 0.3626
Epoch [6], last_lr: 0.00089, train_loss: 1.1127, val_loss: 2.2660, val_acc: 0.4001, f1: 0.4001
Epoch [7], last_lr: 0.00097, train_loss: 1.0324, val_loss: 2.3319, val_acc: 0.3908, f1: 0.3908
Epoch [8], last_lr: 0.00100, train_loss: 0.9625, val_loss: 2.1734, val_acc: 0.4344, f1: 0.4344
Epoch [9], last_lr: 0.00099, train_loss: 0.9156, val_loss: 2.1451, val_acc: 0.4203, f1: 0.4203
Epoch [10], last_lr: 0.00098, train_loss: 0.8761, val_loss: 1.9783, val_acc: 0.4526, f1: 0.4526
Epoch [11], last_lr: 0.00095, train_loss: 0.8443, val_loss: 1.9231, val_acc: 0.4737, f1: 0.4737
Epoch [12], last_lr: 0.00091, train_loss: 0.8225, val_loss: 1.8758, val_acc: 0.4863, f1: 0.4863
Epoch [13], last_lr: 0.00087, train_loss: 0.7922, val_loss: 2.0651, val_acc: 0.4554, f1: 0.4554
Epoch [14], last_lr: 0.00081, train_loss: 0.7682, val_loss: 1.7566, val_acc: 0.5232, f1: 0.5232
Epoch [15], last_lr: 0.00075, train_loss: 0.7328, val_loss: 1.6660, val_acc: 0.5340, f1: 0.5340
Epoch [16], last_lr: 0.00068, train_loss: 0.7037, val_loss: 1.7159, val_acc: 0.5249, f1: 0.5249
Epoch [17], last_lr: 0.00061, train_loss: 0.6701, val_loss: 1.5830, val_acc: 0.5587, f1: 0.5587
Epoch [18], last_lr: 0.00054, train_loss: 0.6307, val_loss: 1.4818, val_acc: 0.5832, f1: 0.5832
Epoch [19], last_lr: 0.00046, train_loss: 0.5902, val_loss: 1.2993, val_acc: 0.6300, f1: 0.6300
Epoch [20], last_lr: 0.00039, train_loss: 0.5501, val_loss: 1.3164, val_acc: 0.6258, f1: 0.6258
Epoch [21], last_lr: 0.00032, train_loss: 0.4994, val_loss: 1.1746, val_acc: 0.6645, f1: 0.6645
Epoch [22], last_lr: 0.00025, train_loss: 0.4483, val_loss: 1.1173, val_acc: 0.6808, f1: 0.6808
Epoch [23], last_lr: 0.00019, train_loss: 0.3930, val_loss: 1.0913, val_acc: 0.6892, f1: 0.6892
Epoch [24], last_lr: 0.00013, train_loss: 0.3362, val_loss: 1.0296, val_acc: 0.7095, f1: 0.7095
Epoch [25], last_lr: 0.00009, train_loss: 0.2866, val_loss: 0.9549, val_acc: 0.7290, f1: 0.7290
Epoch [26], last_lr: 0.00005, train_loss: 0.2347, val_loss: 0.9431, val_acc: 0.7327, f1: 0.7327
Epoch [27], last_lr: 0.00002, train_loss: 0.1990, val_loss: 0.9160, val_acc: 0.7437, f1: 0.7437
Epoch [28], last_lr: 0.00001, train_loss: 0.1752, val_loss: 0.9061, val_acc: 0.7439, f1: 0.7439
Epoch [29], last_lr: 0.00000, train_loss: 0.1656, val_loss: 0.9096, val_acc: 0.7444, f1: 0.7444
Training time: 958.24 s
