Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /home/ds7000/.local/lib/python3.8/site-packages (1.3.2)
Requirement already satisfied: joblib>=1.1.1 in /home/ds7000/.local/lib/python3.8/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ds7000/.local/lib/python3.8/site-packages (from scikit-learn) (3.2.0)
Requirement already satisfied: numpy<2.0,>=1.17.3 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.19.2)
Requirement already satisfied: scipy>=1.5.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/scipy-1.5.2-py3.8-linux-x86_64.egg (from scikit-learn) (1.5.2)
WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.
You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.
Files already downloaded and verified
Files already downloaded and verified
/home/ds7000/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch [0], last_lr: 0.00007, train_loss: 0.9491, val_loss: 1.8262, val_acc: 0.3496, f1: 0.3496
Epoch [1], last_lr: 0.00015, train_loss: 0.7620, val_loss: 1.3795, val_acc: 0.4974, f1: 0.4974
Epoch [2], last_lr: 0.00028, train_loss: 0.6543, val_loss: 1.2544, val_acc: 0.5553, f1: 0.5553
Epoch [3], last_lr: 0.00044, train_loss: 0.5685, val_loss: 1.0625, val_acc: 0.6255, f1: 0.6255
Epoch [4], last_lr: 0.00060, train_loss: 0.4866, val_loss: 0.9401, val_acc: 0.6646, f1: 0.6646
Epoch [5], last_lr: 0.00076, train_loss: 0.4269, val_loss: 1.0577, val_acc: 0.6416, f1: 0.6416
Epoch [6], last_lr: 0.00089, train_loss: 0.3767, val_loss: 0.8827, val_acc: 0.6938, f1: 0.6937
Epoch [7], last_lr: 0.00097, train_loss: 0.3406, val_loss: 0.8542, val_acc: 0.7255, f1: 0.7255
Epoch [8], last_lr: 0.00100, train_loss: 0.3150, val_loss: 0.8283, val_acc: 0.7121, f1: 0.7121
Epoch [9], last_lr: 0.00099, train_loss: 0.2940, val_loss: 0.6364, val_acc: 0.7821, f1: 0.7821
Epoch [10], last_lr: 0.00098, train_loss: 0.2765, val_loss: 0.7031, val_acc: 0.7662, f1: 0.7662
Epoch [11], last_lr: 0.00095, train_loss: 0.2663, val_loss: 0.8532, val_acc: 0.7224, f1: 0.7224
Epoch [12], last_lr: 0.00091, train_loss: 0.2566, val_loss: 0.6411, val_acc: 0.7802, f1: 0.7802
Epoch [13], last_lr: 0.00087, train_loss: 0.2434, val_loss: 0.6186, val_acc: 0.7895, f1: 0.7895
Epoch [14], last_lr: 0.00081, train_loss: 0.2374, val_loss: 0.5548, val_acc: 0.8157, f1: 0.8157
Epoch [15], last_lr: 0.00075, train_loss: 0.2258, val_loss: 0.5393, val_acc: 0.8133, f1: 0.8133
Epoch [16], last_lr: 0.00068, train_loss: 0.2128, val_loss: 0.6169, val_acc: 0.7911, f1: 0.7911
Epoch [17], last_lr: 0.00061, train_loss: 0.2028, val_loss: 0.4752, val_acc: 0.8438, f1: 0.8438
Epoch [18], last_lr: 0.00054, train_loss: 0.1875, val_loss: 0.4184, val_acc: 0.8583, f1: 0.8583
Epoch [19], last_lr: 0.00046, train_loss: 0.1722, val_loss: 0.4146, val_acc: 0.8626, f1: 0.8626
Epoch [20], last_lr: 0.00039, train_loss: 0.1586, val_loss: 0.4233, val_acc: 0.8592, f1: 0.8592
Epoch [21], last_lr: 0.00032, train_loss: 0.1455, val_loss: 0.3620, val_acc: 0.8800, f1: 0.8800
Epoch [22], last_lr: 0.00025, train_loss: 0.1251, val_loss: 0.3267, val_acc: 0.8899, f1: 0.8899
Epoch [23], last_lr: 0.00019, train_loss: 0.1072, val_loss: 0.3044, val_acc: 0.8985, f1: 0.8985
Epoch [24], last_lr: 0.00013, train_loss: 0.0906, val_loss: 0.2766, val_acc: 0.9099, f1: 0.9099
Epoch [25], last_lr: 0.00009, train_loss: 0.0731, val_loss: 0.2638, val_acc: 0.9177, f1: 0.9177
Epoch [26], last_lr: 0.00005, train_loss: 0.0556, val_loss: 0.2365, val_acc: 0.9266, f1: 0.9266
Epoch [27], last_lr: 0.00002, train_loss: 0.0456, val_loss: 0.2313, val_acc: 0.9295, f1: 0.9295
Epoch [28], last_lr: 0.00001, train_loss: 0.0377, val_loss: 0.2309, val_acc: 0.9278, f1: 0.9278
Epoch [29], last_lr: 0.00000, train_loss: 0.0344, val_loss: 0.2274, val_acc: 0.9302, f1: 0.9302
Training time: 758.91 s
